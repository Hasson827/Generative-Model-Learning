# 信息论基础

## 熵(Entropy)

**定义1（熵）**：一个分布为$p(X)$的离散型随机变量$X$的熵为
$$
H(X) = \sum_{x}{p(x)\log{\frac{1}{p(x)}}} = -\sum_{x}{p(x)\log{p(x)}}
$$
其中约定$0\log{\frac{1}{0}]} = 0$，当对数的底取2时，熵的单位为比特；当对数的底取$e$时，熵的单位为奈特。

熵是对随机变量不确定性的度量——熵越大，表示该随机变量的不确定性越大。这里的$\log{\frac{1}{p(x)}}$衡量了观察到值$x$的吃惊程度——小概率事件发生时，吃惊程度更高；当一个值几乎确定出现时，$p(x)\approx1$，吃惊程度近似为0。熵实际上是平均意义下的吃惊程度，具有如下基本性质：

1. $$
   H(X)\ge 0
   $$

2. $$
   H(X)\le\log{|X|},\text{其中等号成立当且仅当}X\text{服从均匀分布}
   $$

   

对于离散型随机变量，熵也可以被称为香农熵。而对于连续性随机变量，我们用可微熵(Differential Entropy)衡量概率密度函数描述的不确定性。

**定义2（可微熵）**：连续型随机变量$X$的概率密度函数为$p(x)$，其可微熵为
$$
H(p) = \mathbb{E}\left[ -\log{p(x)} \right] = -\int{p(x)\log{p(x)}dx}
$$


可微熵与香农熵具有类似的形式，但是却有不同的特性。例如，可微熵的取值可能为负值

## 互信息

熵的概念可以扩展到多变量的联合分布，定义联合熵、条件熵和互信息。

**定义3（联合熵）**：两个离散型随机变量$X$和$Y$的联合熵为
$$
H(X,Y) = \mathbb{E}_{p(x,y)}\left[ \log{\frac{1}{p(x,y)}} \right]
$$
**定义4（条件熵）**：给定随机变量$Y$的取值$y$，随机变量$X$的条件熵为
$$
H(X|Y=y) = \mathbb{E}_{p(x|y)}\left[\frac{1}{\log{p(x|y)}}\right]
$$
条件熵是度量在观察到$Y$取值之后，随机变量$X$的不确定性。值得注意的是，$H(X|Y=y)$可能比$H(X)$大，即观察到$Y$的取值后，$X$可能变得更加不确定。当不确定$Y$的具体取值时，其条件熵定义为
$$
H(X|Y) = \mathbb{E}_{p(y)}[H(X|Y=y)]
$$
即对所有$Y$的取值求期望。根据定义，可以得到熵的链式法则：
$$
H(X,Y) = H(X)+H(Y|X) = H(Y)+H(X|Y)
$$
可以证明，$H(X)\ge H(X|Y)$，其中等号成立当且仅当$X$和$Y$相互独立。

直观解释：$H(X)$是对随机变量$X$的不确定性的一种度量。$H(X|Y)$是在知道另外一个变量$Y$的前提下，对$X$剩余的不确定性。如果我们知道了一些额外的信息（比如$Y$），那么我们对$X$的不确定性应该**不会增加** 。也就是说，知道更多信息后，$X$的“不确定程度”应该减少或不变。

**定义5（互信息）**：两个离散型随机变量$X$和$Y$的互信息为
$$
I(X,Y) = \mathbb{E}_{p(x,y)}\left[ \log{\frac{p(x,y)}{p(x)p(y)}} \right]
$$
根据定义，可以推导出互信息的一些性质：

1. $$
   I(X,Y) = I(Y,X)
   $$

2. $$
   I(X,Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y)
   $$

## 相对熵

对刻画随机变量$X$的两个概率分布$p(X)$和$q(X)$，可以用相对熵度量它们之间的差异。

**定义6（相对熵）**：概率分布$p(X)$和$q(X)$的相对熵为
$$
KL(p,q) = \mathbb{E}_{p(x)}\left[ \log{\frac{p(x)}{q(x)}} \right]
$$
该相对熵也称为$p(X)$和$q(X)$之间的KL(Kullbak-Leibler)散度。KL散度适用于离散型和连续型随机变量，满足如下性质：

**定义1（信息不等式）**：对任意两个分布$p(X)$和$q(X)$，有
$$
KL(p,q)\ge0
$$
其中等号成立当且仅当$p=q$，即$\forall x,\; p(x) = q(x)$。